{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "---\n",
    "#### Import libraries and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>laurtay7166</td>\n",
       "      <td>[Routine Help] Suggestions for dehydrated to n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atrevz</td>\n",
       "      <td>[B&amp;amp;A] Did the Fifty Shades of Snail sebace...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                              title selftext  \\\n",
       "0  laurtay7166  [Routine Help] Suggestions for dehydrated to n...      NaN   \n",
       "1       atrevz  [B&amp;A] Did the Fifty Shades of Snail sebace...      NaN   \n",
       "\n",
       "   num_comments  score          subreddit  is_ab  \n",
       "0             1      1  skincareaddiction      0  \n",
       "1             1      1  skincareaddiction      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/skincare.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude removed and deleted posts\n",
    "---\n",
    "Excluding removed and deleted posts from my data because it will serve no value and misrepresent the subreddits in my analysis or modeling since it's no longer on the forum.\n",
    "\n",
    "I have 64,008 (64%) left of data after this elimination, which  still leaves me with ample amount of observations to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 64008 (64%) left of my data to work with.\n"
     ]
    }
   ],
   "source": [
    "df = df[(df['selftext'] != '[removed]') & (df['selftext'] != '[deleted]')]\n",
    "\n",
    "print(f'I have {df.shape[0]} ({round((df.shape[0])/100_000*100)}%) left of my data to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, title, selftext, num_comments, score, subreddit, is_ab]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['selftext'] == '[removed]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values\n",
    "---\n",
    "\n",
    "There are missing selftexts in my data.\n",
    "\n",
    "After investigating each subreddit to get more context, I found that the two subreddits have differing posting culture. Some users in the AsianBeauty community post product reviews with a title and a written review in the comments ([example](https://www.reddit.com/r/AsianBeauty/comments/g3kd2a/review_11_cosrx_products/)). Hence, it is showing up as missing values in my data. However, since there are other posts to represent [product reviews](https://www.reddit.com/r/AsianBeauty/?f=flair_name%3A%22Review%22) that includes selftexts, I will be eliminating these posts from my data.\n",
    "\n",
    "More, in the SkincareAddiction subreddit, the posts without selftexts are usually a product question for the community to chime in, indicated by the tag and  title ([example](https://www.reddit.com/r/SkincareAddiction/comments/dd3za5/product_question_cleansers_and_toners_that_help/)). Considering the myriad of [product questions](https://www.reddit.com/r/SkincareAddiction/?f=flair_name%3A%22Product%20Question%22) with selftexts that will be able to represent this type of post, I will also remove this from my data.\n",
    "\n",
    "After excluding posts with missing `selftext`s, I have 32,226 (32%) left of my data, which is still ample amount of data to work with. Class balance is to be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author              0\n",
       "title               0\n",
       "selftext        31782\n",
       "num_comments        0\n",
       "score               0\n",
       "subreddit           0\n",
       "is_ab               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 32226 (32%) left of my data to work with.\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "print(f'I have {df.shape[0]} ({round((df.shape[0])/100_000*100)}%) left of my data to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author          0\n",
       "title           0\n",
       "selftext        0\n",
       "num_comments    0\n",
       "score           0\n",
       "subreddit       0\n",
       "is_ab           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data types\n",
    "---\n",
    "All data types are correct, no changes made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author          object\n",
       "title           object\n",
       "selftext        object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "subreddit       object\n",
       "is_ab            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates\n",
    "---\n",
    "\n",
    "After doing further investigation by browsing the duplicate posts on Reddit.com, I concluded that most of these are either spam posts that have long been removed/deleted, or scheduled posts from an automated moderator (see Fig 1 - 4 at the bottom of this notebook).\n",
    "\n",
    "The appearance of words from duplicate posts will skew my analysis, thus I will remove them from my data.\n",
    "\n",
    "After removing these duplicates, I have 31,618 (32%) left of my data, which is ample amount of observations to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55677</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52983</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70699</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64817</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66820</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67938</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65788</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83263</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Fluff and Hauls</td>\n",
       "      <td>Post your meme trash, gifs, hauls, sheet mask ...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55774</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62069</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>Daily Deals, Fluff, and Hauls</td>\n",
       "      <td>Post all of your deals, memes, gifs, hauls, sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                          title  \\\n",
       "55677  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "52983  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "70699  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "64817  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "66820  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "67938  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "65788  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "83263  AutoModerator          Daily Fluff and Hauls   \n",
       "55774  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "62069  AutoModerator  Daily Deals, Fluff, and Hauls   \n",
       "\n",
       "                                                selftext  num_comments  score  \\\n",
       "55677  Post all of your deals, memes, gifs, hauls, sh...             6      1   \n",
       "52983  Post all of your deals, memes, gifs, hauls, sh...             3      1   \n",
       "70699  Post all of your deals, memes, gifs, hauls, sh...             8      3   \n",
       "64817  Post all of your deals, memes, gifs, hauls, sh...             2      6   \n",
       "66820  Post all of your deals, memes, gifs, hauls, sh...             6      1   \n",
       "67938  Post all of your deals, memes, gifs, hauls, sh...             2      1   \n",
       "65788  Post all of your deals, memes, gifs, hauls, sh...            18      1   \n",
       "83263  Post your meme trash, gifs, hauls, sheet mask ...            13      5   \n",
       "55774  Post all of your deals, memes, gifs, hauls, sh...             0      1   \n",
       "62069  Post all of your deals, memes, gifs, hauls, sh...             1      3   \n",
       "\n",
       "         subreddit  is_ab  \n",
       "55677  asianbeauty      1  \n",
       "52983  asianbeauty      1  \n",
       "70699  asianbeauty      1  \n",
       "64817  asianbeauty      1  \n",
       "66820  asianbeauty      1  \n",
       "67938  asianbeauty      1  \n",
       "65788  asianbeauty      1  \n",
       "83263  asianbeauty      1  \n",
       "55774  asianbeauty      1  \n",
       "62069  asianbeauty      1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Investigate these common titles on Reddit.com\n",
    "\n",
    "See the screenshots of my findings in Fig 1 - 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55126</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55497</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55803</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56221</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56324</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56326</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56565</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56608</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit\n",
       "55126  asianbeauty\n",
       "55497  asianbeauty\n",
       "55803  asianbeauty\n",
       "56221  asianbeauty\n",
       "56324  asianbeauty\n",
       "56326  asianbeauty\n",
       "56565  asianbeauty\n",
       "56608  asianbeauty"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='DISCOUNT CODE: \"KINDNESS\" for an additional discount on yesstyle.com :D &lt;3'][['subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45355</th>\n",
       "      <td>skincareaddiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45358</th>\n",
       "      <td>skincareaddiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               subreddit\n",
       "45355  skincareaddiction\n",
       "45358  skincareaddiction"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='when to apply overnight exfoliator?'][['subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>skincareaddiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8866</th>\n",
       "      <td>skincareaddiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>skincareaddiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subreddit\n",
       "8801  skincareaddiction\n",
       "8866  skincareaddiction\n",
       "8867  skincareaddiction"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='[Skin Concerns] Red, itchy skin on body'][['subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79272</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79313</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79349</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79396</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79430</th>\n",
       "      <td>asianbeauty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit\n",
       "79272  asianbeauty\n",
       "79313  asianbeauty\n",
       "79349  asianbeauty\n",
       "79396  asianbeauty\n",
       "79430  asianbeauty"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='Daily Fluff and Hauls'][['subreddit']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 31618 (32%) left of my data to work with.\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f'I have {df.shape[0]} ({round((df.shape[0])/100_000*100)}%) left of my data to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove posts from AutoModerator\n",
    "---\n",
    "I became aware of automated posts since investigating duplicate observations in my data. However, not all AutoModerator posts are flagged as duplicates because oftentimes they are labeled with date of posting (see codes below). \n",
    "\n",
    "Because posts by AutoModerators are meant to catalyze community engagement where users share their skincare thoughts and concerns in the comments instead, I will remove these posts to have better insights about the community rather than the moderator.\n",
    "\n",
    "After removing AutoModerator posts, I have 19,923 (20%) left of my data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get a sense of AutoModerator's post `title`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automoderator_posts(col_name, str_keywords):\n",
    "    for post in df[(df['author'] == 'AutoModerator') & (df[col_name].str.contains(str_keywords))][col_name]:\n",
    "        print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Personal] It's Casual Friday! General Chat thread - Apr 17, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Apr 10, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Apr 03, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Mar 27, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Mar 20, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Mar 13, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Mar 06, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Feb 28, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Feb 21, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Feb 14, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Feb 07, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Jan 31, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Jan 24, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Jan 17, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Jan 10, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Jan 03, 2020\n",
      "[Personal] It's Casual Friday! General Chat thread - Dec 27, 2019\n",
      "[Personal] It's Casual Friday! General Chat thread - Dec 20, 2019\n"
     ]
    }
   ],
   "source": [
    "automoderator_posts('title', \"It's Casual Friday!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-Haul Monthly April 23, 2020\n",
      "Anti-Haul Monthly March 26, 2020\n",
      "Anti-Haul Monthly February 27, 2020\n",
      "Anti-Haul Monthly January 23, 2020\n",
      "Anti-Haul Monthly December 26, 2019\n",
      "Anti-Haul Monthly November 28, 2019\n",
      "Anti-Haul Monthly October 24, 2019\n",
      "Anti-Haul Monthly September 26, 2019\n",
      "Anti-Haul Monthly August 22, 2019\n",
      "Anti-Haul Monthly July 25, 2019\n",
      "Anti-Haul Monthly June 27, 2019\n",
      "Anti-Haul Monthly May 23, 2019\n",
      "Anti-Haul Monthly April 25, 2019\n",
      "Anti-Haul Monthly March 28, 2019\n",
      "Anti-Haul Monthly February 28, 2019\n"
     ]
    }
   ],
   "source": [
    "automoderator_posts('title', \"Anti-Haul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 29153 (29%) left of my data to work with.\n"
     ]
    }
   ],
   "source": [
    "df = df[df['author'] != 'AutoModerator']\n",
    "\n",
    "print(f'I have {df.shape[0]} ({round((df.shape[0])/100_000*100)}%) left of my data to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author, title, selftext, num_comments, score, subreddit, is_ab]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['author'] == 'AutoModerator']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and handle class imbalance\n",
    "---\n",
    "\n",
    "After cleaning my data this far, the split in my target variable, `is_ab`, is 30 - 70. Which means that there is class imbalance in my data.\n",
    "\n",
    "I'm keeping 9,000 random samples from SkincareAddiction subreddit to approximately balance the 8,871 observations I have from AsianBeauty.\n",
    "\n",
    "I have 17,871 (18%) left of my data, which is just enough observations to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skincareaddiction    20282\n",
       "asianbeauty           8871\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keep only 9K random samples from majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 17871 (18%) left of my data to work with.\n"
     ]
    }
   ],
   "source": [
    "skin = df[df['is_ab'] == 0].sample(9000)\n",
    "azn = df[df['is_ab'] == 1]\n",
    "\n",
    "df = pd.concat([azn, skin], axis=0)\n",
    "\n",
    "print(f'I have {df.shape[0]} ({round((df.shape[0])/100_000*100)}%) left of my data to work with.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skincareaddiction    0.503609\n",
       "asianbeauty          0.496391\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text variables\n",
    "---\n",
    "Here, I'm cleaning both `title` and `selftext` variables from unneccessary words, characters, numbers, even URL's to help filter out the noise from our data. \n",
    "\n",
    "I initially cleaned the texts from preset NLTK stopwords. However, after seeing how frequent trivial and obvious words are in EDA as well as the strongest coefficient weights in my first Logistic Regression model, I added a list of custom stopwords to remove noise words to get deeper and more meaningful insights about the subreddit communities in EDA as well as modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom stopwords list\n",
    "\n",
    "I'm pickling a list of top coefficient weights from 1st Logistic Regression model (Notebook: `../code/004_model_lg1.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_coef_features = pickle.load(open('../assets/sorted_features_lg_model1.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create custom stopwords list \n",
    "\n",
    "Create a list of custom stopwords based on the list of top coefficient weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_to_add = [\n",
    "    # Noise\n",
    "    'discussion',\n",
    "    'beauty',\n",
    "    'http',\n",
    "    'do',\n",
    "    'you',\n",
    "    'nice',\n",
    "    'favourite',\n",
    "    'where',\n",
    "    'care',\n",
    "    'routine',\n",
    "    'your',\n",
    "    'edit',\n",
    "    'tried',\n",
    "    'product',\n",
    "    'products',\n",
    "    'shop',\n",
    "    'fluff',\n",
    "    'items',\n",
    "    'post',\n",
    "    'pack',\n",
    "    'power',\n",
    "    'com',\n",
    "    'www',\n",
    "    'black',\n",
    "    'friday',\n",
    "    'free',\n",
    "    'shipping',\n",
    "    'help',\n",
    "    'removed',\n",
    "    'guys',\n",
    "    'buy',\n",
    "    'like',\n",
    "    'really',\n",
    "    've',\n",
    "    'use',\n",
    "    'help',\n",
    "    'just',\n",
    "    'using',\n",
    "    'don',\n",
    "    'know',\n",
    "    'hg',\n",
    "    'ebay',\n",
    "    'skin',\n",
    "    'face',\n",
    "    'amp',\n",
    "    \n",
    "    # Obvious signal\n",
    "    'ab',\n",
    "    'abers',\n",
    "    'asian','asians',\n",
    "    'korean',\n",
    "    'japanese',\n",
    "    'korea',\n",
    "    'japan',\n",
    "    'yesstyle',\n",
    "    'jolse',\n",
    "    'hong','kong',\n",
    "    'soko','glam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile lists stopwords, including custom, NLTK, and CountVectorizer stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvec_stopwords = list(CountVectorizer(stop_words = 'english').get_stop_words())\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "custom_stopwords = stopwords_to_add + cvec_stopwords + nltk_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickle copiled list for ease of recall in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../assets/custom_stopwords.pkl'\n",
    "\n",
    "pickle.dump(custom_stopwords, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data from irrelevant characters and noise words\n",
    "\n",
    "##### Control text with stopword: 'asian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belong to us Asians :\n",
      "\n",
      "- https://old.reddit.com/r/awcmovement/comments/9uc9i5/only_asians_look_like_anime_characters_only/\n",
      "\n",
      "- https://old.reddit.com/r/awcmovement/comments/9uc93h/only_asians_look_like_anime_characters_only/\n",
      "\n",
      "- https://old.reddit.com/r/awcmovement/comments/9uc8mo/only_asians_look_like_anime_characters_only/\n",
      "\n",
      "- https://old.reddit.com/r/awcmovement/comments/9uc84q/only_asians_look_like_anime_characters_only/\n",
      "\n",
      "- https://old.reddit.com/r/awcmovement/comments/9uc7b8/only_asians_look_like_anime_characters_only/\n",
      "\n",
      "- And hundreds millions others out there waiting for us Asian guys to play with them.\n",
      "\n",
      "The biggest evidence we Asian guys got the best girls can be directly seen on how they look like, the only reason they can be/look like that is because they inherit our Asian genetics when their mother, mother's mothers and so on chose to be inseminated by us Asians guys, of course their mother, mother's mother also not look much different from them when they are young. If our Asian girls chose to be inseminated by western males losers, african males, etc other males then they will not look like that but will like uncute, too masculine, look too old western girls and other non Asian girls. About eye-rolling banana explosion only occur when the girl gives the guy intense pleasure and that is affected by how attractive they are, the more attractive especially innocent the girl then the more powerful the banana will explode.\n",
      "\n",
      "Anyway, the 1st fundriser will get mod status on r/awcmovement and 1 article slot on www.antiwesterncosplayers.blogspot.be , this is the only chance because the next mod have to be fundrisers who contribute 10.000 yen on the fundrising. The mod status are limited for :\n",
      "\n",
      "- Can approve and remove post.\n",
      "\n",
      "- Can make flair.\n",
      "\n",
      "- Etc other than deleting the sub.\n",
      "\n",
      "As for the article slot, it have to do with the anti western cosplayers movement's purpose and it will be there for until 3 days before the end of this year. The fundrising are now on secure link http://www.antiwesterncosplayers.blogspot.be/2018/02/anti-western-cosplayers-movement.html\n",
      "\n",
      "Join the anti western cosplayers movement if you don't want :\n",
      "\n",
      "1. Racist western cosplayers keep doing racism against us Asians. When racist western cosplayers cosplaying our Anime characters are the same as they imitate us Asians because we Asians make our Anime characters based on us Asians which Japanese also agreed with http://www.antiwesterncosplayers.blogspot.be/2018/09/japanese-spoken-up-against-racist.html , imitating other race is racism.\n",
      "\n",
      "2. Racist western cosplayers keep ruining Anime including the Anime characters you love. When racist western cosplayers cosplaying Anime characters they only end up ruining the characters because they look nothing like Anime characters because they are not Asian while Anime characters are based on us Asians http://www.antiwesterncosplayers.blogspot.be/2015/10/you-westerners-are-not-cute-too_22.html , millions of Anime fans especially in Japan are hurt by racist western cosplayers.\n",
      "https://image.ibb.co/dfJV5U/Only_Asians_Look_Like_Anime_Characters.jpg\n",
      "\n",
      "3. More racist trash such as trump to emerge, the anti western cosplayers movement are actually teaching western youth about how bad racism is, at least they will become aware about to not doing racism.\n",
      "\n",
      "The minimum is 500 yen, 100 yen is around 1 usd and only Asians can participate in the fundrising, if you are not Asian you can still participate by joining the circles https://plus.google.com/117850404800846575560 and add more likes.\n",
      "\n",
      "I plan to change the sub status from restricted to public too when there is another mod. As for the 2nd, 3rd and so on fundrisers will only get approved submitter status. I plan to award the next mod status only for those who contribute 10.000 yen on the fundrising. As for the article slot will no longer available on any way because this is the only chance for that.\n",
      "\n",
      "Know that you must not join the fundrising because you want to be a mod on r/awcmovement or to have an article posted on the anti western cosplayers movement site but simply because you agree with the anti western cosplayers movement. The mod status and 1 article slot is just a bonus.\n",
      "\n",
      "**Paypal are accepted**\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[68619,'selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function that cleans our text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    get_text = BeautifulSoup(raw_text).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', letters_only, flags=re.MULTILINE)\n",
    "    words = text.lower().split()\n",
    "    stops = set(custom_stopwords)\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "# 5.03 Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define a function that returns cleaned text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text_columns(df, col_name):\n",
    "\n",
    "    indices = list(df.index)\n",
    "    \n",
    "    for i in indices:\n",
    "        df.loc[i, col_name] = clean_text(df.loc[i, col_name])\n",
    "    \n",
    "    return df[col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean `title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://buyh3re.myshopify.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3926176/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50009    working seasoned estheticians hundreds consult...\n",
       "50014                                           wash water\n",
       "50019                                 favorite repurchased\n",
       "50020                                              times w\n",
       "50034    recommendations minimal ingredient effective h...\n",
       "                               ...                        \n",
       "49252                   misc skincare new year resolutions\n",
       "23438    concerns tips facemasks tightening discolorati...\n",
       "31458                       improve skincare pores redness\n",
       "2257                             fixed damaged barrier yay\n",
       "22390    question new olay body wash b hyaluronic point...\n",
       "Name: title, Length: 17871, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_columns(df, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean `selftext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.yesstyle.com/en/home.html?rco=3RZFC2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.leatheraddicts.com/leather-bondage/real-top-grain-blueblack-leather-full-set-of-75-bondage-restraints/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/a/5L61J0V\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://stone-of-heaven.com/blogs/natural-stones/facial-roller-benefits\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://youtu.be/WHwMM53UGwo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.youtube.com/channel/UCfRBnBAzBhd844x7i7w94Pw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.google.com/search?q=soko+glam.dewytree&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en-us&amp;client=safari\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://imgur.com/gallery/5sOEkm8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/k5GgIpW\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://instagram.com/p/BWeVoAlnHYq/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://mailchi.mp/933be4c1ca4b/a-letter-from-beautysesh?e=7b80cf37d9\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"http://m.beautyboxkorea.com/product/cosrx-one-step-pimple-clear-pad-70p-one-step-moisture-up-pad-70p/14282/?cate_no=1&amp;display_group=3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://m.imgur.com/gallery/gqX6E7k\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/a/eXDvFgK\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://zerostore.com.au/collections/skincare/products/australian-natural-soap-co-calamine-zinc-face-cleanser-bar\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://hellogiggles.com/beauty/skin/muac-chemical-peel-acne-scars-review/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://biossance.com/pages/about-lactic-acid-night-serum\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.nytimes.com/2020/01/20/style/quit-skin-care-routine.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://i.imgur.com/jGYdbva.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/a/7AAnGG3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://i.imgur.com/OtmX7eV.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/VFFW9HI\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/a/akPa3gp\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/gallery/h6sMP4L\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/gallery/aYjtDOy\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/gallery/A3FY6rO\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.amazon.com/OGX-Ever-Straightening-Brazillian-Keratin-Conditioner/dp/B004KATJVC/ref=mp_s_a_1_8?keywords=ogx+conditioner&amp;qid=1578045590&amp;sprefix=ogx+cond&amp;sr=8-8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://www.wired.com/story/whats-deal-with-sunscreen-does-it-work-or-not/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:357: UserWarning: \"https://imgur.com/a/FA6lx4J\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50009    project glowism https glowism friend female en...\n",
       "50014    hello wondering wash water usually refrained w...\n",
       "50019    personal favorites mugwort mask calms helps fi...\n",
       "50020    recently purchased cosrx advanced snail cream ...\n",
       "50034                                                title\n",
       "                               ...                        \n",
       "49252    hello sca thought interesting start skincare n...\n",
       "23438    heard benefits facemasks used im year old male...\n",
       "31458    imgur jufiyr think pictures speak redness zone...\n",
       "2257     type concerns year old lighter tan think combi...\n",
       "22390    trying body wash dullness tone arms legs affor...\n",
       "Name: selftext, Length: 17871, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_columns(df, 'selftext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check:\n",
    "\n",
    "This is the same text as the control text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belong https old reddit r awcmovement comments uc look anime characters https old reddit r awcmovement comments uc h look anime characters https old reddit r awcmovement comments uc mo look anime characters https old reddit r awcmovement comments uc q look anime characters https old reddit r awcmovement comments uc b look anime characters hundreds millions waiting play biggest evidence got best girls directly seen look reason look inherit genetics mother mother mothers chose inseminated course mother mother mother look different young girls chose inseminated western males losers african males males look uncute masculine look old western girls non girls eye rolling banana explosion occur girl gives guy intense pleasure affected attractive attractive especially innocent girl powerful banana explode st fundriser mod status r awcmovement article slot antiwesterncosplayers blogspot chance mod fundrisers contribute yen fundrising mod status limited approve remove make flair deleting sub article slot anti western cosplayers movement purpose days end year fundrising secure link antiwesterncosplayers blogspot anti western cosplayers movement html join anti western cosplayers movement want racist western cosplayers racism racist western cosplayers cosplaying anime characters imitate make anime characters based agreed antiwesterncosplayers blogspot spoken racist html imitating race racism racist western cosplayers ruining anime including anime characters love racist western cosplayers cosplaying anime characters end ruining characters look anime characters anime characters based antiwesterncosplayers blogspot westerners cute html millions anime fans especially hurt racist western cosplayers https image ibb dfjv u look anime characters jpg racist trash trump emerge anti western cosplayers movement actually teaching western youth bad racism aware racism minimum yen yen usd participate fundrising participate joining circles https plus google add likes plan change sub status restricted public mod nd rd fundrisers approved submitter status plan award mod status contribute yen fundrising article slot longer available way chance join fundrising want mod r awcmovement article posted anti western cosplayers movement site simply agree anti western cosplayers movement mod status article slot bonus paypal accepted\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[68619,'selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset index\n",
    "---\n",
    "Resetting indices of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as .csv file\n",
    "---\n",
    "Once I save my cleaned data as a .csv file, I ran into missing values in my EDA. After exploring further in the codes below, these are caused by the syntax cleaning that substituted noise words or URL's with nothing, `''`. \n",
    "\n",
    "You will see that I drop missing values following reading my data in the subsequent notebook(s) since these values will inevitably be rendered as `NaN`.\n",
    "\n",
    "##### Save cleaned data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cleaned_skincare.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm missing values are blank cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17871, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = pd.read_csv('../data/cleaned_skincare.csv')\n",
    "check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking missing `title`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>lilsozy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seen changes country orders got changed australia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NamakaJewelry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>namaka jonathan sayeb kristina ganina jonathan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>liachen03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>canada means lot specific hard b n love magic ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>talarkadeh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>img lpxctm https talarkadeh articles wedding g...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Outrageous-World</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ingredients look different curel intensive moi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17493</th>\n",
       "      <td>Mpos072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>social media seeing people aha bha peeling sol...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>acikwofi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi received azelaic acid plant derived hemi sq...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17508</th>\n",
       "      <td>Universalhoed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suffering texture hate skins texture exactly a...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17613</th>\n",
       "      <td>heathaleatha</td>\n",
       "      <td>NaN</td>\n",
       "      <td>months postpartum breastfeeding trying revamp ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17811</th>\n",
       "      <td>Verize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hi glad great community hope past years used s...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>skincareaddiction</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author title  \\\n",
       "64              lilsozy   NaN   \n",
       "65        NamakaJewelry   NaN   \n",
       "94            liachen03   NaN   \n",
       "124          talarkadeh   NaN   \n",
       "142    Outrageous-World   NaN   \n",
       "...                 ...   ...   \n",
       "17493           Mpos072   NaN   \n",
       "17497          acikwofi   NaN   \n",
       "17508     Universalhoed   NaN   \n",
       "17613      heathaleatha   NaN   \n",
       "17811            Verize   NaN   \n",
       "\n",
       "                                                selftext  num_comments  score  \\\n",
       "64     seen changes country orders got changed australia             1      1   \n",
       "65     namaka jonathan sayeb kristina ganina jonathan...             1      1   \n",
       "94     canada means lot specific hard b n love magic ...             0      1   \n",
       "124    img lpxctm https talarkadeh articles wedding g...             1      1   \n",
       "142    ingredients look different curel intensive moi...             0      1   \n",
       "...                                                  ...           ...    ...   \n",
       "17493  social media seeing people aha bha peeling sol...            16      1   \n",
       "17497  hi received azelaic acid plant derived hemi sq...             2      1   \n",
       "17508  suffering texture hate skins texture exactly a...             3      1   \n",
       "17613  months postpartum breastfeeding trying revamp ...             1      1   \n",
       "17811  hi glad great community hope past years used s...             4      1   \n",
       "\n",
       "               subreddit  is_ab  \n",
       "64           asianbeauty      1  \n",
       "65           asianbeauty      1  \n",
       "94           asianbeauty      1  \n",
       "124          asianbeauty      1  \n",
       "142          asianbeauty      1  \n",
       "...                  ...    ...  \n",
       "17493  skincareaddiction      0  \n",
       "17497  skincareaddiction      0  \n",
       "17508  skincareaddiction      0  \n",
       "17613  skincareaddiction      0  \n",
       "17811  skincareaddiction      0  \n",
       "\n",
       "[138 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[check['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[64, 'title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking missing `selftext`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-daifuku-</td>\n",
       "      <td>kikumasamune hadalabo premium lotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>JuliaOphelia</td>\n",
       "      <td>alternative curology asia southeast asia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>pizzaoven12</td>\n",
       "      <td>quaaludes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>asianbeauty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                     title selftext  \\\n",
       "51      -daifuku-      kikumasamune hadalabo premium lotion      NaN   \n",
       "317  JuliaOphelia  alternative curology asia southeast asia      NaN   \n",
       "990   pizzaoven12                                 quaaludes      NaN   \n",
       "\n",
       "     num_comments  score    subreddit  is_ab  \n",
       "51             37      1  asianbeauty      1  \n",
       "317             1      1  asianbeauty      1  \n",
       "990             1      1  asianbeauty      1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[check['selftext'].isna()][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[51, 'selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screenshots for reference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 1.\n",
    ">DISCOUNT CODE: \"KINDNESS\" for an additional discount on yesstyle.com :D <3\n",
    "\n",
    "<img src=\"../assets/duplicate1.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "##### Fig 2.\n",
    ">when to apply overnight exfoliator?\n",
    "\n",
    "<img src=\"../assets/duplicate2.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "##### Fig 3.\n",
    ">[Skin Concerns] Red, itchy skin on body\n",
    "\n",
    "<img src=\"../assets/duplicate3.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "##### Fig 4.\n",
    ">[Skin Concerns] Daily Fluff and Hauls\n",
    "\n",
    "<img src=\"../assets/duplicate4.png\" width=\"50%\" height=\"50%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
